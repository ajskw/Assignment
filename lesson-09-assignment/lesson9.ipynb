{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131db075-4619-4651-bffd-5c72f669f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 双向最大匹配分词方法实现\n",
    "class BiMM():\n",
    "    def __init__(self, user_dict):\n",
    "        self.user_dict = user_dict\n",
    "    def cut(self, sentence):\n",
    "        \"\"\"\n",
    "        正向最大匹配（FMM）\n",
    "        :param user_dict: 词典\n",
    "        :param sentence: 句子\n",
    "        \"\"\"\n",
    "        # 词典中最长词长度\n",
    "        max_len = max([len(item) for item in self.user_dict])\n",
    "        start = 0\n",
    "        f_single_word_num = 0 #单字词数量计数\n",
    "        f_segs = [] #存储分词结果\n",
    "        while start != len(sentence):\n",
    "            index = start+max_len\n",
    "            if index>len(sentence):\n",
    "                index = len(sentence)\n",
    "            for i in range(max_len):\n",
    "                if (sentence[start:index] in self.user_dict) or (len(sentence[start:index])==1):\n",
    "                    f_segs.append(sentence[start:index])\n",
    "                    if index-start==1:\n",
    "                        f_single_word_num+=1\n",
    "                    #print(sentence[start:index], end='/')\n",
    "                    start = index\n",
    "                    break\n",
    "                index += -1\n",
    "        \"\"\"\n",
    "        反向最大匹配（BMM）\n",
    "        :param user_dict:词典\n",
    "        :param sentence:句子\n",
    "        \"\"\"\n",
    "        result = [] #暂存分词结果\n",
    "        b_segs = [] #存储分词结果\n",
    "        b_single_word_num = 0 #单字词数量计数\n",
    "        start = len(sentence)\n",
    "        while start != 0:\n",
    "            index = start - max_len\n",
    "            if index < 0:\n",
    "                index = 0\n",
    "            for i in range(max_len):\n",
    "                #print(sentence[index:start])\n",
    "                if (sentence[index:start] in self.user_dict) or (len(sentence[index:start])==1):\n",
    "                    result.append(sentence[index:start])\n",
    "                    if start-index == 1:\n",
    "                        b_single_word_num+=1\n",
    "                    start = index\n",
    "                    break\n",
    "                index += 1\n",
    "        b_segs = result[::-1]\n",
    "    \n",
    "        \"\"\"\n",
    "        双向最大匹配\n",
    "        \"\"\"\n",
    "        if len(f_segs)<len(b_segs):\n",
    "            return f_segs\n",
    "        elif len(f_segs)>len(b_segs):\n",
    "            return b_segs\n",
    "        else:\n",
    "            if f_single_word_num>b_single_word_num:\n",
    "                return b_segs\n",
    "            else:\n",
    "                return f_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a519c94e-e3e8-41ac-9e06-d688e7d80a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我们', '在', '野生动物园', '玩']\n"
     ]
    }
   ],
   "source": [
    "#测试双向最大匹配分词方法\n",
    "user_dict = ['我们','在', '在野', '生动', '野生', '动物园', '野生动物园', '物','园','玩']\n",
    "sentence = '我们在野生动物园玩'\n",
    "bimm = BiMM(user_dict)\n",
    "print(bimm.cut(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e44a1ff-30ff-4cb1-bd3c-7a1b8340e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56482 47 22111\n"
     ]
    }
   ],
   "source": [
    "#从人民日报分词语料中读取分词数据，并构建词典\n",
    "#将人民日报分词语料还原成原文\n",
    "ribao_dict = [] #语料中出现的所有词\n",
    "ribao_dict_sorted = [] #过滤掉低频词后的词典\n",
    "yuanwen = [] #存储语料原文\n",
    "punctuation = []  #存储标点符号\n",
    "c = 0\n",
    "with open(r\"C:\\Users\\Administrator.DESKTOP-TSVP15E\\Downloads\\199801 (2).txt\",\"r\",encoding=\"utf-8\") as fr, \\\n",
    "open(r\"C:\\Users\\Administrator.DESKTOP-TSVP15E\\Downloads\\199801_source.txt\",\"w\",encoding=\"utf-8\") as fw:\n",
    "    for line in fr:\n",
    "        #if c > 100:\n",
    "        #    break\n",
    "        ls = line.strip().split('  ')\n",
    "        line_segs = []\n",
    "        for i in range(1,len(ls)):\n",
    "            #print(ls[i])\n",
    "            #if '/' in ls[i]:\n",
    "            end_idx = ls[i].index('/')\n",
    "            line_segs.append(ls[i][:end_idx])\n",
    "            ribao_dict.append(ls[i][:end_idx])\n",
    "        \n",
    "            if ls[i].endswith('/w'):\n",
    "                punctuation.append(ls[i][:end_idx])\n",
    "        fw.write(''.join(line_segs)+'\\n')\n",
    "        yuanwen.append(''.join(line_segs))\n",
    "        #c+=1\n",
    "word_count_dict = {}\n",
    "for word in ribao_dict:\n",
    "    if word not in word_count_dict:\n",
    "        word_count_dict[word]=1\n",
    "    else:\n",
    "        word_count_dict[word]+=1\n",
    "sorted_word_count = sorted(word_count_dict.items(), key = lambda item:item[1], reverse=True)\n",
    "\n",
    "#保存出现频次大于2的词，存入词典\n",
    "for k,v in sorted_word_count:\n",
    "    if v>2:\n",
    "        ribao_dict_sorted.append(k)\n",
    "ribao_dict_set = set(ribao_dict)\n",
    "punctuation = set(punctuation)\n",
    "print(len(ribao_dict_set),len(punctuation), len(ribao_dict_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccbb8631-c3c7-4fde-8962-c936a4a1283a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['迈向', '新', '世纪']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用新词典测试双向最大匹配分词方法\n",
    "bimm = BiMM(ribao_dict_sorted)\n",
    "bimm.cut(\"迈向新世纪\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a4113ff-0b1c-4b32-b30c-581c42fe8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segging: 100%|█████████████████████████████████████████████████████████████████| 23064/23064 [5:50:12<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21012.682418107986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm #进度条线上模块\n",
    "time_start = time.time() #计时\n",
    "source_text = []\n",
    "with open(r\"C:\\Users\\Administrator.DESKTOP-TSVP15E\\Downloads\\199801_source.txt\",\"r\",encoding=\"utf-8\") as fr:\n",
    "    for line in fr:\n",
    "        source_text.append(line.strip())\n",
    "\n",
    "bimm = BiMM(ribao_dict_sorted)\n",
    "with open(r\"C:\\Users\\Administrator.DESKTOP-TSVP15E\\Downloads\\199801_source_segs.txt\",\"w\",encoding=\"utf-8\") as fw:\n",
    "    for t in tqdm(range(len(source_text)),desc = 'segging'):\n",
    "        text = source_text[t]  \n",
    "        start_idx = 0\n",
    "        Bi_segs = []\n",
    "        flag = True\n",
    "        for idx in range(len(text)):\n",
    "            if text[idx] in punctuation: #有标点符号的，先根据标点符号节分句子\n",
    "                Bi_segs.extend(bimm.cut(text[start_idx:idx+1]))\n",
    "                start_idx = idx+1\n",
    "                flag = False\n",
    "        if flag:\n",
    "            Bi_segs.extend(bimm.cut(text))\n",
    "        fw.write('/  '.join(Bi_segs)+'\\n')\n",
    "time_end = time.time()\n",
    "print(time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dc396c7-5564-4f44-8f8d-843583b4e540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ... 其他函数 ...\n",
    "\n",
    "def generate_text(trigrams, bigrams, start_word, length=100):\n",
    "    sentence = [start_word, None]  # 初始化句子为包含至少两个元素的列表\n",
    "    while len(sentence) < length:\n",
    "        bigram = (sentence[-2], sentence[-1])\n",
    "        if bigram not in trigrams or not trigrams[bigram]:\n",
    "            # 如果没有更多的TriGram或Bigram，则停止生成\n",
    "            # 同时检查是否已经生成了句子结束符\n",
    "            if sentence[-1] == '.':\n",
    "                break\n",
    "            # 如果句子不完整，尝试添加句子结束符并停止生成\n",
    "            sentence.append('.')\n",
    "            break\n",
    "        \n",
    "        # 根据概率选择下一个词\n",
    "        next_word, count = random.choices(list(trigrams[bigram].keys()), weights=trigrams[bigram].values())[0]\n",
    "        sentence.append(next_word)\n",
    "        \n",
    "        # 如果生成了句子结束符，也停止生成\n",
    "        if next_word == '.':\n",
    "            break\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def main():\n",
    "    file_path = \"C:\\\\Users\\\\Administrator.DESKTOP-TSVP15E\\\\Downloads\\\\199801_source.txt\"\n",
    "    text = read_data(file_path)\n",
    "    words = preprocess_text(text)\n",
    "    bigrams, trigrams = build_trigram_model(words)\n",
    "    \n",
    "    # 选择一个随机的BiGram作为起始点，并确保起始词是bigram的第一个词\n",
    "    start_bigram = random.choice(list(bigrams.keys()))\n",
    "    start_word = start_bigram[0]  # 确保start_word是bigram的第一个词\n",
    "    \n",
    "    # 确保start_word是trigrams中的有效起始词\n",
    "    while (start_word, start_bigram[1]) not in trigrams or not trigrams[(start_word, start_bigram[1])]:\n",
    "        start_bigram = random.choice(list(bigrams.keys()))\n",
    "        start_word = start_bigram[0]\n",
    "    \n",
    "    generated_sentence = generate_text(trigrams, bigrams, start_word)\n",
    "    print(generated_sentence)\n",
    "\n",
    "# ... 主程序 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40202bc3-a501-472f-8a6b-8590cc309e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我和老伴都是贫苦出生，都是共产党员，在对待贫困群众的态度上，我们是一致的。我每次下乡时，妻子总是在衣服里装上几百元钱，说是遇事好应急。凡是见到脚上有泥土，穿得不好的乡下人，她都特别热情，从不叫客人脱鞋进屋。客人有什么事，她都用笔认真记下来，贫困群众送的土特产，她怕伤了他们的自尊，一律折价换物。有时因我拿出去接济贫困群众的钱太多，家里生活无法保障，她也从不吭一声，而是先找他人借钱，等下月工资发了再还。说实在的，没有妻子的支持，我先后将家里省吃俭用节约下来的５万多元钱接济给贫困群众是难以做到的。妻子对两个孩子要求也非常严格。这几年孩子大了，懂事了，看到家里比同学家都穷，问妈妈：“我们家的钱到哪去了？”她告诉孩子说：“我们有很多乡下的穷亲戚，都支持他们了。”我们夫妻俩都没有了父母，人民就是我们的父母，孝敬父母是做儿女的本分啊！ 尉健行说，一九九八年是全面贯彻落实党的十五大提出的各项任务的第一年。做好新的一年的党风廉政建设和反腐败工作，对于保证党的十五大确定的目标和任务的顺利实现，具有重要意义。 十一、开庭审判 各乡镇都成立了读报用报活动领导小组，制定了报刊传阅制度，全县５１４个行政村中除建立阅览室外，还在村里人口集中的地方建起了阅报栏，指定专人定期更换。 据新华社瓦莱塔１月１２日电（记者袁锦林）马耳他总统乌戈·米夫萨德·邦尼奇１２日上午在总统府会见了正在这里访问的[中国国务院副总理兼外长钱其琛。 而那乳名，在乡音中生长 近几年来，该院先后投资数千万元，新建了住院大楼，引进了全身ＣＴ、磁共振，１９９６年又引进了血流透析机、平板运动试验及Ｈｏｔｅｒ等心脏检测设备。 李德润葛洪\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "from collections import defaultdict, deque  \n",
    "import random  \n",
    "  \n",
    "def read_file(file_path):  \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:  \n",
    "        return file.read().split()  # 假设文件已经是分词后的形式  \n",
    "  \n",
    "def build_trigram_dict(tokens):  \n",
    "    trigrams = defaultdict(list)  \n",
    "    for i in range(len(tokens) - 2):  \n",
    "        trigram = (tokens[i], tokens[i+1], tokens[i+2])  \n",
    "        trigrams[trigram[:2]].append(trigram[2])  \n",
    "    return trigrams  \n",
    "  \n",
    "def train_trigram_model(trigrams):  \n",
    "    model = defaultdict(lambda: defaultdict(int))  \n",
    "    total_counts = defaultdict(int)  \n",
    "      \n",
    "    for (w1, w2), w3s in trigrams.items():  \n",
    "        for w3 in w3s:  \n",
    "            model[(w1, w2)][w3] += 1  \n",
    "            total_counts[(w1, w2)] += 1  \n",
    "      \n",
    "    # Normalize probabilities  \n",
    "    for (w1, w2), count in total_counts.items():  \n",
    "        for w3, freq in model[(w1, w2)].items():  \n",
    "            model[(w1, w2)][w3] /= count  \n",
    "      \n",
    "    return model  \n",
    "  \n",
    "def generate_sentence(model, start_trigram, num_words, unk_token='<UNK>'):  \n",
    "    sentence = list(start_trigram)  \n",
    "    history = deque(start_trigram, maxlen=2)  \n",
    "    word_list = set(tokens)  # 假设tokens是预处理后的所有单词列表  \n",
    "  \n",
    "    for _ in range(num_words - 2):  \n",
    "        next_word_probs = model[tuple(history)]  \n",
    "        if not next_word_probs:  # 如果字典为空  \n",
    "            next_word = random.choice(list(word_list))  # 从所有词中随机选择一个  \n",
    "        else:  \n",
    "            next_word = random.choices(list(next_word_probs.keys()), list(next_word_probs.values()), k=1)[0]  \n",
    "        sentence.append(next_word)  \n",
    "        history.append(next_word)  \n",
    "      \n",
    "    return ' '.join(sentence)\n",
    "  \n",
    "# 使用示例  \n",
    "file_path = \"C:\\\\Users\\\\Administrator.DESKTOP-TSVP15E\\\\Downloads\\\\199801_source.txt\"  \n",
    "tokens = read_file(file_path)  \n",
    "trigrams = build_trigram_dict(tokens)  \n",
    "model = train_trigram_model(trigrams)  \n",
    "# 假设我们有一个初始的TriGram  \n",
    "start_trigram = ()  \n",
    "generated_sentence = generate_sentence(model, start_trigram, 10)  \n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec6afe36-875c-49d3-877b-d6a24c1ee0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词: ['19980101', '-', '01', '001', ' ']\n"
     ]
    }
   ],
   "source": [
    "import jieba  # 假设使用jieba进行中文分词  \n",
    "import networkx as nx  # 需要在文件顶部导入networkx  \n",
    "  \n",
    "def tokenize(text):    \n",
    "    return list(jieba.cut(text))  \n",
    "  \n",
    "def build_graph(words):    \n",
    "    # 假设 words 是一个分词后的单词列表    \n",
    "    graph = nx.Graph()    \n",
    "    # 在这里添加节点和边来构建图  \n",
    "    # 这里只添加节点作为示例，你可能需要添加边的逻辑  \n",
    "    for word in words:    \n",
    "        graph.add_node(word)    \n",
    "    return graph  \n",
    "  \n",
    "# 假设有一个计算PageRank得分的函数，但这里我们只是一个示例，你需要自己实现或找到实现  \n",
    "def pagerank(graph):  \n",
    "    # 这里应该使用networkx的pagerank算法来计算节点的PageRank得分  \n",
    "    # 注意：networkx的pagerank函数需要图作为输入，并返回一个字典，其中键是节点，值是PageRank得分  \n",
    "    return nx.pagerank(graph)  \n",
    "  \n",
    "# 示例用法  \n",
    "text = \"C:\\\\Users\\\\Administrator.DESKTOP-TSVP15E\\\\Desktop\\\\199801(4).txt\"\n",
    "with open(text, 'r', encoding='utf-8') as f:  # 打开文件并读取内容  \n",
    "    content = f.read()  \n",
    "words = tokenize(content)  # 分词  \n",
    "graph = build_graph(words)  # 构建词图  \n",
    "scores = pagerank(graph)  # 计算PageRank得分  \n",
    "  \n",
    "# 提取得分最高的五个关键词  \n",
    "top_keywords = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]  \n",
    "print(\"关键词:\", [word for word, _ in top_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0410229-a6c7-45cc-8d70-f24cdd30f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词: [' ', '迈向', '充满', '希望', '的']\n"
     ]
    }
   ],
   "source": [
    "import jieba  # 假设使用jieba进行中文分词  \n",
    "import networkx as nx  # 需要在文件顶部导入networkx  \n",
    "  \n",
    "def tokenize(text):    \n",
    "    return list(jieba.cut(text))  \n",
    "  \n",
    "def build_graph(words):    \n",
    "    # 假设 words 是一个分词后的单词列表    \n",
    "    graph = nx.Graph()    \n",
    "    # 在这里添加节点和边来构建图  \n",
    "    # 这里只添加节点作为示例，你可能需要添加边的逻辑  \n",
    "    for word in words:    \n",
    "        graph.add_node(word)    \n",
    "    return graph  \n",
    "  \n",
    "# 假设有一个计算PageRank得分的函数，但这里我们只是一个示例，你需要自己实现或找到实现  \n",
    "def pagerank(graph):  \n",
    "    # 这里应该使用networkx的pagerank算法来计算节点的PageRank得分  \n",
    "    # 注意：networkx的pagerank函数需要图作为输入，并返回一个字典，其中键是节点，值是PageRank得分  \n",
    "    return nx.pagerank(graph)  \n",
    "  \n",
    "# 示例用法  \n",
    "text = \"C:\\\\Users\\\\Administrator.DESKTOP-TSVP15E\\\\Desktop\\\\199801(5).txt\"\n",
    "with open(text, 'r', encoding='utf-8') as f:  # 打开文件并读取内容  \n",
    "    content = f.read()  \n",
    "words = tokenize(content)  # 分词  \n",
    "graph = build_graph(words)  # 构建词图  \n",
    "scores = pagerank(graph)  # 计算PageRank得分  \n",
    "  \n",
    "# 提取得分最高的五个关键词  \n",
    "top_keywords = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]  \n",
    "print(\"关键词:\", [word for word, _ in top_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0966b0fb-a7a2-4a3b-9fa2-16fcf8b9874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词: ['中国', '发展', '国际', '人民', '世界']\n"
     ]
    }
   ],
   "source": [
    "import jieba  \n",
    "  \n",
    "# 读取文件内容  \n",
    "with open(\"C:\\\\Users\\\\Administrator.DESKTOP-TSVP15E\\\\Desktop\\\\199801(5).txt\", encoding='utf-8') as f:  \n",
    "    text = f.read().strip()  \n",
    "  \n",
    "# 使用jieba进行分词，并转换为列表  \n",
    "seg_list = jieba.cut(text, cut_all=False)  \n",
    "words = list(seg_list)  \n",
    "  \n",
    "# 创建一个字典来存储词频  \n",
    "word_freq = {}  \n",
    "for word in words:  \n",
    "    if len(word) > 1:  # 过滤掉单字（可选）  \n",
    "        if word not in word_freq:  \n",
    "            word_freq[word] = 1  \n",
    "        else:  \n",
    "            word_freq[word] += 1  \n",
    "  \n",
    "# 按照词频进行排序  \n",
    "sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)  \n",
    "  \n",
    "# 假设我们选择前五个词作为关键词  \n",
    "top_keywords = [word for word, freq in sorted_words[:5]]  \n",
    "print(\"关键词:\", top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8a293-223a-4619-920d-6ded9685f2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
